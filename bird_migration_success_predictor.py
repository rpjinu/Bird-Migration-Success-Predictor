# -*- coding: utf-8 -*-
"""Bird Migration Success Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19DzwZ4Y5RZzcgdCWLXvd8SmDmScLcfIR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
import pickle
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
#Warnings
import warnings
warnings.filterwarnings('ignore')

"""#***Load Dataset:-***"""

df=pd.read_csv("/content/bird_migration_data.csv")

df.head()

df.shape

"""#**EDA(Explotary Data Analysis):-**"""

df.info()

df.describe()

#Check the null values
df.isnull().sum()

df['Interrupted_Reason'].value_counts()

#some of the null values present in Interrupted_Reason replace with no reason
df['Interrupted_Reason'].fillna('No Reason',inplace=True)

#check the duplicated
df.duplicated().sum()

df.dtypes

#drop the bird_id
df.drop('Bird_ID',axis=1,inplace=True)

#check all categorical column
cat_column=df.select_dtypes(include='object').columns
cat_column

#Numerical column list
num_column=df.select_dtypes(exclude='object').columns
num_column

"""#***Data visualizations:-***"""

#dataset visualization
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='Region', hue='Species')
plt.title('Species Distribution Across Regions')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#data visualization the 'Interrupted_Reason'
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='Interrupted_Reason',palette='viridis')
plt.title('Interrupted Reasons')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.countplot(data=df, y='Migration_Reason', order=df['Migration_Reason'].value_counts().index)
plt.title("Migration Reasons")
plt.tight_layout()
plt.show()

#data  visualization of all categorical column
for i in cat_column:
  plt.figure(figsize=(10, 6))
  sns.countplot(data=df, x=i,palette='viridis')
  plt.title(f'{i} Distribution')
  plt.xticks(rotation=45)
  plt.tight_layout()
  plt.show()

#Numerical column distribution column visualization
for i in num_column:
  plt.figure(figsize=(10, 6))
  sns.histplot(data=df, x=i, kde=True,palette='viridis')
  plt.title(f'{i} Distribution')
  plt.tight_layout()
  plt.show()

#check as heatmap
monthly_counts = df.groupby(['Migration_Start_Month', 'Migration_End_Month']).size().unstack(fill_value=0)

plt.figure(figsize=(10, 8))
sns.heatmap(monthly_counts, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Start vs End Month of Migration (Heatmap)')
plt.xlabel('End Month')
plt.ylabel('Start Month')
plt.tight_layout()
plt.show()

#check which month high migration in n bar chart
monthly_counts = df['Migration_Start_Month'].value_counts().sort_index()

plt.figure(figsize=(10, 6))
plt.bar(monthly_counts.index, monthly_counts.values)
plt.title('Monthly Migration Counts')
plt.xlabel('Month')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df.columns

df['Tagged_By'].value_counts()

#drop the column unncessary feature that not required
# List of columns to drop
columns_to_drop = [
    'Tag_Battery_Level_%', 'Signal_Strength_dB', 'Tracking_Quality',
    'Observation_Counts', 'Observation_Quality', 'Tagged_By', 'Recovery_Location_Known',
    'Recovery_Time_days','Tag_Type','Tag_Weight_g','Start_Latitude','Start_Longitude','End_Latitude','End_Longitude','Max_Altitude_m','Min_Altitude_m',
    'Migration_Interrupted', 'Interrupted_Reason', 'Nesting_Success',
    'Flock_Size', 'Rest_Stops', 'Predator_Sightings'
]

# Drop the columns
df_cleaned = df.drop(columns=columns_to_drop)

# Preview the cleaned dataset
print(df_cleaned.columns)

df_cleaned.shape







"""#***Spliting the dataset:-***"""

#spliting the dataset X,y
X=df_cleaned.drop('Migration_Success',axis=1)
y=df_cleaned['Migration_Success']

cate_col=X.select_dtypes(include='object').columns
cate_col

"""#***Label Encoding:-***"""

#convert the categorical dataset in to numerical column
# Label Encode categorical columns
label_encoders = {}
for col in cate_col:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le  # Save encoders for later use

#Save the label encoding
with open('label_encoders.pkl', 'wb') as encoder_file:
    pickle.dump(label_encoders, encoder_file)

y.value_counts()

#convert the categorical dataset in target column
y.replace({'Successful':1,'Failed':0},inplace=True)

y.value_counts()

#check the dtype
y.dtype

X.dtypes

"""#***Spliting the dataset in to Train and Test:-***"""

#Split the dataset to train and test
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

#Check the shape of dataset
X_train.shape,X_test.shape,y_train.shape,y_test.shape

y_train.head()

X_train.head()

"""#***Model Building:-***"""

#use for loop model to train
models = {
    "RandomForest": RandomForestClassifier(random_state=42),
    "GradientBoosting": GradientBoostingClassifier(random_state=42),
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "DecisionTree": DecisionTreeClassifier(random_state=42),
    "SVC": SVC(probability=True),
    "NaiveBayes": GaussianNB(),
    "KNeighbors": KNeighborsClassifier()
}

# Train and evaluate
best_model = None
best_accuracy = 0
best_model_name = ""

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    print(f"{name} Accuracy: {acc:.4f}")

    if acc > best_accuracy:
        best_accuracy = acc
        best_model = model
        best_model_name = name

print(f"\nâœ… Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}")

"""#***Save Best Model:-***"""

# Save best model and encoders
with open("best_migration_model.pkl", "wb") as model_file:
    pickle.dump(best_model, model_file)



